"""
**This script was generated by an LLM**
Video Pose Estimation Demo
This script uses a trained YOLO segmentation model to estimate camera pose
from a video file and save the annotated results with path tracing to an output file.
"""

import cv2
import numpy as np
import argparse
from ultralytics import YOLO
from pose_estimation_demo import PoseEstimator

# Define available models and their paths
models_to_paths = {
    '1': '/Users/sebnico/Desktop/CIS4900/injection-tracker/weights/120-epochs-Oct-8/best.pt',
    '2': '/Users/sebnico/Desktop/CIS4900/injection-tracker/weights/second_dataset/640/best_train6.pt',
    '3': '/Users/sebnico/Desktop/CIS4900/injection-tracker/weights/second_dataset/1280/best_train12.pt',
    '4': '/Users/sebnico/Desktop/CIS4900/injection-tracker/weights/second_dataset/2560/best_train15.pt',
}


def parse_arguments():
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(
        description='Run camera pose estimation on video file and save output with path tracing',
    )
    
    parser.add_argument(
        'model',
        type=str,
        choices=['1', '2', '3', '4'],
        help='Model number to use (1-4)'
    )
    
    parser.add_argument(
        '--video',
        type=str,
        default='demos/clip.mp4',
        help='Path to input video file (default: demos/clip.mp4)'
    )
    
    parser.add_argument(
        '--output',
        type=str,
        required=True,
        help='Path to output video file (required)'
    )
    
    parser.add_argument(
        '--device',
        type=str,
        default='mps',
        choices=['cpu', 'cuda', 'mps'],
        help='Device to run inference on (default: mps)'
    )
    
    parser.add_argument(
        '--conf',
        type=float,
        default=0.25,
        help='Confidence threshold for detections (default: 0.25)'
    )
    
    return parser.parse_args()


def main():
    """Main function to process video with pose estimation and save output."""
    args = parse_arguments()
    
    # Get model path
    model_path = models_to_paths[args.model]
    video_path = args.video
    output_path = args.output
    conf_threshold = args.conf
    
    print(f"Loading model: {model_path}")
    
    # Initialize pose estimator
    estimator = PoseEstimator(model_path)
    
    # Adjust object dimensions based on your actual object size
    estimator.object_width = 100.0   # millimeters (adjust as needed)
    estimator.object_height = 100.0  # millimeters (adjust as needed)
    
    print(f"Processing video: {video_path}")
    
    # Open video file
    cap = cv2.VideoCapture(video_path)
    
    if not cap.isOpened():
        print(f"Error: Could not open video file {video_path}")
        return
    
    # Get video properties
    fps = int(cap.get(cv2.CAP_PROP_FPS))
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    
    print(f"Video properties: {width}x{height} @ {fps} fps, {total_frames} frames")
    
    # Setup camera matrix from video dimensions
    estimator.setup_camera_matrix(width, height)
    
    # Setup video writer
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
    
    print(f"Saving output to: {output_path}")
    print("Processing frames...")
    
    frame_count = 0
    
    # Smoothing parameters
    alpha = 0.3  # Smoothing factor
    smoothed_rvec = None
    smoothed_tvec = None
    
    # Path tracing
    show_path = True
    max_path_length = 100
    camera_path = []
    
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        
        frame_count += 1
        
        # Run segmentation
        results = estimator.model(frame, conf=conf_threshold, verbose=False)
        
        # Process results
        for result in results:
            # Draw segmentation masks
            annotated_frame = result.plot()
            
            # Get masks and estimate pose
            if result.masks is not None:
                # Try to get polygon coordinates first (preferred)
                try:
                    mask_polygons = result.masks.xy.cpu().numpy()
                except:
                    # Fallback to binary masks
                    masks = result.masks.data.cpu().numpy()
                    
                    # Convert masks to polygons for better keypoint extraction
                    mask_polygons = []
                    for mask in masks:
                        # Convert mask to binary image
                        h, w = mask.shape
                        mask_binary = (mask * 255).astype(np.uint8)
                        
                        # Find contour
                        contours, _ = cv2.findContours(
                            mask_binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE
                        )
                        if contours:
                            # Use largest contour
                            largest = max(contours, key=cv2.contourArea)
                            mask_polygons.append(largest.reshape(-1, 2))
                
                # Estimate pose
                if len(mask_polygons) > 0:
                    pose = estimator.estimate_pose(frame, mask_polygons)
                    
                    if pose is not None:
                        rvec, tvec = pose
                        
                        # Smooth the pose estimates
                        if smoothed_rvec is None:
                            smoothed_rvec = rvec.copy()
                            smoothed_tvec = tvec.copy()
                        else:
                            smoothed_rvec = (alpha * smoothed_rvec) + ((1 - alpha) * rvec)
                            smoothed_tvec = (alpha * smoothed_tvec) + ((1 - alpha) * tvec)
                        
                        # Draw path trace
                        if show_path:
                            origin_3d = np.float32([[0, 0, 0]])
                            origin_2d, _ = cv2.projectPoints(
                                origin_3d, smoothed_rvec, smoothed_tvec,
                                estimator.camera_matrix, estimator.dist_coeffs
                            )
                            
                            h, w = annotated_frame.shape[:2]
                            x, y = origin_2d[0][0].astype(int)
                            camera_path.append((w - x, h - y))
                            
                            if len(camera_path) > max_path_length:
                                camera_path.pop(0)
                            
                            # Draw path
                            if len(camera_path) > 1:
                                base_color = (0, 0, 255)  # Red in BGR format
                                for i in range(1, len(camera_path)):
                                    if len(camera_path) > 2:
                                        age_factor = (i - 1) / (len(camera_path) - 2)
                                    else:
                                        age_factor = 1.0
                                    brightness = 0.4 + age_factor * 0.6
                                    color = tuple(int(c * brightness) for c in base_color)
                                    cv2.line(
                                        annotated_frame,
                                        camera_path[i-1],
                                        camera_path[i],
                                        color,
                                        10
                                    )
                        
                        # Draw white ball at last path position
                        if show_path and len(camera_path) > 0:
                            ball_pos = camera_path[-1]
                            cv2.circle(annotated_frame, ball_pos, 20, (255, 255, 255), -1)  # Increased from 8 to 15 for larger ball
            
            # Write frame to output video
            out.write(annotated_frame)
        
        # Print progress
        if frame_count % 30 == 0:
            progress = (frame_count / total_frames) * 100
            print(f"Progress: {frame_count}/{total_frames} frames ({progress:.1f}%)")
    
    # Cleanup
    cap.release()
    out.release()
    
    print(f"Finished! Output saved to: {output_path}")


if __name__ == '__main__':
    main()